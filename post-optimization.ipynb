{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-keeping",
   "metadata": {},
   "source": [
    "## Neuromorphic HAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-membership",
   "metadata": {},
   "source": [
    "### By loading optimized (hyper)parameters and weights, the confusion matrices can be here obtained, together with memory and energy evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-hopkins",
   "metadata": {},
   "source": [
    "#### <b>IMPORTANT NOTES:</b>\n",
    "<b>1)</b> for each network,  variables <b>optim_nni_experiment</b> and <b>optim_nni_trial</b> must be set accordingly to the IDs of the NNI optimization experiment and trial whose results are to be used;<br>\n",
    "<b>2)</b> in the case of the sCNN, these variables are used to keep the optimized structure parameters as for the <i>NON-SPIKING</i> counterpart. The NNI optimization experiment and trial IDs for the spiking CNN must be set through the variables <b>snn_nni_experiment</b> and <b>snn_nni_trial</b> respectively;<br>\n",
    "<b>3)</b> NNI experiment results (of each trial) can be found in:<br>\n",
    "&emsp;&emsp;{os.path.expanduser('~')}<br>\n",
    "&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&emsp;&emsp;&emsp;&emsp;| \\_ \\_ nni-experiments<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;| \\_ \\_  {experiment ID}<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;| \\_ \\_  trials<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;| \\_ \\_  {trial ID}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-junior",
   "metadata": {},
   "source": [
    "<b>Hyperlinks to each network:</b><br>\n",
    "<b>[LSTM](#Section_1)</b><br>\n",
    "<b>[CNN](#Section_2)</b><br>\n",
    "<b>[sCNN](#Section_3)</b><br>\n",
    "<b>[LMU](#Section_4)</b><br>\n",
    "<b>[sLMU](#Section_5)</b><br>\n",
    "<b>[LMU (ff)](#Section_6)</b><br>\n",
    "<b>[sLMU (ff)](#Section_7)</b><br>\n",
    "[FLOPs calculation and energy estimation for LMU and LMU (ff)](#Section_8)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import random as rn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.signal import butter, freqz\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "from nengo.utils.filter_design import cont2discrete\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2,l1\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMUCell(nengo.Network):\n",
    "    def __init__(self, units, order, theta, input_d, tau, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        Q = np.arange(order, dtype=np.float64)\n",
    "        R = (2 * Q + 1)[:, None] / theta\n",
    "        j, i = np.meshgrid(Q, Q)\n",
    "\n",
    "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R \n",
    "        B = (-1.0) ** Q[:, None] * R \n",
    "        C = np.ones((1, order))\n",
    "        D = np.zeros((1,))\n",
    "\n",
    "        A, B, _, _, _ = cont2discrete((A, B, C, D), dt=tau, method=\"zoh\") # original: dt=1.0\n",
    "\n",
    "        A_H = 1/(1-np.exp(-1/tau)) * (A - np.exp(-1/tau)*np.identity(order))\n",
    "        B_H = 1/(1-np.exp(-1/tau)) * B\n",
    "\n",
    "\n",
    "        with self:\n",
    "            nengo_dl.configure_settings(trainable=None)\n",
    "\n",
    "            # create objects corresponding to the x/u/m/h\n",
    "            self.x = nengo.Node(size_in=input_d)\n",
    "            self.u = nengo.Node(size_in=1)\n",
    "            self.m = nengo.Node(size_in=order)\n",
    "            self.h = nengo_dl.TensorNode(tf.nn.tanh, shape_in=(units,), pass_time=False)\n",
    "\n",
    "            # compute u_t:\n",
    "            # e_x\n",
    "            nengo.Connection(\n",
    "                self.x, self.u, transform=np.ones((1, input_d)), synapse=None\n",
    "            )\n",
    "            \n",
    "            # e_h\n",
    "            nengo.Connection(\n",
    "                self.h, self.u, transform=np.ones((1, units)), synapse=0\n",
    "            )\n",
    "            \n",
    "            # e_m\n",
    "            nengo.Connection(\n",
    "                self.m, self.u, transform=np.ones((1, order)), synapse=0\n",
    "            )\n",
    "\n",
    "            # compute m_t:\n",
    "            conn_A = nengo.Connection(self.m, self.m, transform=A_H, synapse=0)\n",
    "            self.config[conn_A].trainable = True\n",
    "            conn_B = nengo.Connection(self.u, self.m, transform=B_H, synapse=None)\n",
    "            self.config[conn_B].trainable = True\n",
    "\n",
    "            # compute h_t:\n",
    "            nengo.Connection(\n",
    "                self.x, self.h, transform=nengo_dl.dists.Glorot(), synapse=None\n",
    "            )\n",
    "            nengo.Connection(\n",
    "                self.h, self.h, transform=nengo_dl.dists.Glorot(), synapse=0\n",
    "            )\n",
    "            nengo.Connection(\n",
    "                self.m, self.h, transform=nengo_dl.dists.Glorot(), synapse=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wisdm2_data(filename):\n",
    "    filepath = os.path.join('./data/',filename+'.npz')\n",
    "    a = np.load(filepath)\n",
    "    return (a['arr_0'], a['arr_1'], a['arr_2'], a['arr_3'], a['arr_4'], a['arr_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceData:\n",
    "    def __init__(self, sample, fs, channels):\n",
    "        self.data = []\n",
    "        sample = sample.T\n",
    "        for data_axis in range(sample.shape[0]):\n",
    "            self.data.append(sample[data_axis, :])\n",
    "\n",
    "        self.fs = fs\n",
    "        self.freq_range = (0.5, np.floor(self.fs / 2))\n",
    "\n",
    "        freq_min, freq_max = self.freq_range\n",
    "        octave = (channels - 0.5) * np.log10(2) / np.log10(freq_max / freq_min)\n",
    "        self.freq_centr = np.array([freq_min * (2 ** (ch / octave)) for ch in range(channels)])\n",
    "        self.freq_poli = np.array(\n",
    "            [(freq * (2 ** (-1 / (2 * octave))), (freq * (2 ** (1 / (2 * octave))))) for freq in self.freq_centr])\n",
    "        self.freq_poli[-1, 1] = fs / 2 * 0.99999\n",
    "\n",
    "    def decomposition(self, filterbank):\n",
    "        self.components = []\n",
    "        for data_axis in self.data:\n",
    "            tmp = []\n",
    "            for num, den in filterbank:\n",
    "                from scipy.signal import lfilter\n",
    "                tmp.append(lfilter(num, den, data_axis))\n",
    "            self.components.append(tmp)\n",
    "\n",
    "\n",
    "def frequency_decomposition(array, channels=5, fs=20, order=2):\n",
    "\n",
    "    array_dec = []\n",
    "\n",
    "    for ii in range(len(array)):\n",
    "    \n",
    "        sample = DeviceData(array[ii], fs, channels)\n",
    "    \n",
    "        butter_filterbank = []\n",
    "        for fl, fh in sample.freq_poli:\n",
    "            num, den = butter(N=order, Wn=(fl, fh), btype='band', fs=sample.fs)\n",
    "            butter_filterbank.append([num, den])\n",
    "    \n",
    "        sample.decomposition(butter_filterbank)\n",
    "    \n",
    "        features = []\n",
    "        for data_axis in sample.components:\n",
    "            for component in data_axis:\n",
    "                features.append(np.array(component))\n",
    "        features = np.vstack(features)\n",
    "        features = features.T\n",
    "    \n",
    "        array_dec.append(features)\n",
    "\n",
    "    return np.array(array_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy at the end of training\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend(['train_'+string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix_labels(subset):\n",
    "    \n",
    "    act_map = {\n",
    "        'A': 'walking',\n",
    "        'B': 'jogging',\n",
    "        'C': 'stairs',\n",
    "        'D': 'sitting',\n",
    "        'E': 'standing',\n",
    "        'M': 'kicking',\n",
    "        'P': 'dribbling',\n",
    "        'O': 'catch',\n",
    "        'F': 'typing',\n",
    "        'Q': 'writing',\n",
    "        'R': 'clapping',\n",
    "        'G': 'teeth',\n",
    "        'S': 'folding',\n",
    "        'J': 'pasta',\n",
    "        'H': 'soup',\n",
    "        'L': 'sandwich',\n",
    "        'I': 'chips',\n",
    "        'K': 'drinking',\n",
    "    }\n",
    "    \n",
    "    if subset == 1:\n",
    "        labels = list(act_map.values())[:6]\n",
    "    if subset == 2:\n",
    "        labels = list(act_map.values())[6:13]\n",
    "    if subset == 3:\n",
    "        labels = list(act_map.values())[13:]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_footprint(model, nengo=True):\n",
    "    \n",
    "    mem_fp = 0\n",
    "    total = 0\n",
    "    missed = 0\n",
    "    \n",
    "    if nengo:\n",
    "        \n",
    "        model_weights = model.keras_model.weights\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model_weights = model.weights\n",
    "        \n",
    "    for s in model_weights:\n",
    "        if ('32' in str(s.dtype)) or (str(s.dtype)=='int'):\n",
    "            mem = 4*np.prod(s.shape)/1e6 # MB\n",
    "            total += np.prod(s.shape)\n",
    "            mem_fp += mem\n",
    "        elif '64' in str(s.dtype):\n",
    "            mem = 8*np.prod(s.shape)/1e6 # MB\n",
    "            total += np.prod(s.shape)\n",
    "            mem_fp += mem\n",
    "        else:\n",
    "            missed += np.prod(s.shape)\n",
    "    \n",
    "    return mem_fp, total, missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops(model):\n",
    "    \n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function([tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    \n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        \n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sops_LMUens(net, freqdec=False):\n",
    "    \n",
    "    with net:\n",
    "        lmu_inner.add_neuron_output()\n",
    "        p_spikes = nengo.Probe(lmu_inner.neuron_output, label=\"p_spikes\")\n",
    "        net.config[p_spikes].keep_history = True\n",
    "    \n",
    "    with nengo_dl.Simulator(net) as sim:\n",
    "        \n",
    "        sim.load_params(\"./output/tmp_{}_{}_{}/best_test_{}\".format(net_type,optim_nni_experiment,datafile[5:],optim_nni_experiment))\n",
    "    \n",
    "        sops = []\n",
    "        accs = []\n",
    "        \n",
    "        dt = 0.001 # the default value in Nengo\n",
    "    \n",
    "        for ii in range(int(0.1*len(x_test))):\n",
    "            \n",
    "            simulation_steps = int(len(x_test[ii]))\n",
    "    \n",
    "            sim.run_steps(simulation_steps, data={inp: x_test[ii][np.newaxis,:,:]})\n",
    "    \n",
    "        spikes = sim.data[p_spikes]/amplitude*dt\n",
    "        spikes_per_neuron = np.sum(spikes > 0, axis=0)\n",
    "        sops = np.sum(spikes_per_neuron)/int(0.1*len(x_test))\n",
    "    \n",
    "        energy = sops*5.07e-10 # Event-Driven Signal Processing with Neuromorphic Computing Systems, https://ieeexplore.ieee.org/document/9053043/\n",
    "    \n",
    "        num_nn = 0\n",
    "        for ee in net.all_ensembles:\n",
    "            for nn in ee.neurons:\n",
    "                num_nn +=1\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Total number of neurons:\",int(num_nn))\n",
    "        print(\"SOPs:\",int(np.round(np.mean(sops),0)))\n",
    "        print(\"Energy evaluation on Loihi: \"+str(np.round(energy*1e6,2))+\" μJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sops_spikingCNN(net):\n",
    "    \n",
    "    with net:\n",
    "        \n",
    "        dense_p = nengo.Probe(net.layers[model.layers[5].get_output_at(-1)])\n",
    "    \n",
    "    with nengo_dl.Simulator(net) as sim:\n",
    "        \n",
    "        sim.load_params(\"./output/tmp_s{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "    \n",
    "        sops = []\n",
    "        preds = []\n",
    "        accs = []\n",
    "        \n",
    "        dt = 0.001 # the default value in Nengo\n",
    "    \n",
    "        for ii in range(int(0.1*len(tiled_x_test))):\n",
    "            \n",
    "            simulation_steps = int(len(tiled_x_test[ii]))\n",
    "        \n",
    "            sim.run_steps(simulation_steps, data={net.all_nodes[0]: tiled_x_test[ii][np.newaxis,:,:]})\n",
    "    \n",
    "        spikes_conv0 = sim.data[conv0_p]/(1/snn_parameters['nni_keras2snn_network/scale_firing_rates/randint'])*dt\n",
    "        spikes_conv1 = sim.data[conv1_p]/(1/snn_parameters['nni_keras2snn_network/scale_firing_rates/randint'])*dt\n",
    "        spikes_dense = sim.data[dense_p]/(1/snn_parameters['nni_keras2snn_network/scale_firing_rates/randint'])*dt\n",
    "        spikes_per_neuron_conv0 = np.sum(spikes_conv0 > 0, axis=0)\n",
    "        spikes_per_neuron_conv1 = np.sum(spikes_conv1 > 0, axis=0)\n",
    "        spikes_per_neuron_dense = np.sum(spikes_dense > 0, axis=0)\n",
    "        sops = np.sum([np.sum(spikes_per_neuron_conv0), np.sum(spikes_per_neuron_conv1), np.sum(spikes_per_neuron_dense)]) / int(0.1*len(tiled_x_test))\n",
    "    \n",
    "        energy = sops*5.07e-10 # Event-Driven Signal Processing with Neuromorphic Computing Systems, https://ieeexplore.ieee.org/document/9053043/\n",
    "        \n",
    "        num_nn = 0\n",
    "        for ee in net.all_ensembles:\n",
    "            for nn in ee.neurons:\n",
    "                num_nn +=1\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Total number of neurons:\",int(num_nn))\n",
    "        print(\"SOPs:\",int(np.round(np.mean(sops),0)))\n",
    "        print(\"Energy evaluation on Loihi: \"+str(np.round(energy*1e6,2))+\" μJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-occurrence",
   "metadata": {},
   "source": [
    "<a id='Section_1'></a>\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'lstm'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM_{}_subset{}_{}\".format(device,subset,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['nni_network/batch_size/randint']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = y_train_oh\n",
    "y_val = y_val_oh\n",
    "y_test = y_test_oh\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# First LSTM layer\n",
    "model.add(LSTM(network_parameters['nni_network/LSTM_units_1/randint'],\n",
    "               return_sequences=True,\n",
    "               input_shape=(timesteps, input_dim))\n",
    "        )\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(network_parameters['nni_network/LSTM_Dropout_1/quniform']))\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(network_parameters['nni_network/LSTM_units_2/randint'],\n",
    "               recurrent_regularizer=l2(network_parameters['nni_network/LSTM_l2_2/quniform']), \n",
    "               input_shape=(timesteps, input_dim))\n",
    "         ) \n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(network_parameters['nni_network/LSTM_Dropout_2/quniform']))\n",
    "# Adding a dense output layer\n",
    "model.add(Dense(n_classes, activation='softmax')) \n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "optim = Adam(lr=network_parameters['nni_network/lr/quniform'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fp, total, missed = memory_footprint(model, nengo=False)\n",
    "\n",
    "print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "print('Total:',total)\n",
    "print('Missed:',missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FLOPs: {}\".format(get_flops(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = get_flops(model)*7.53e-10 # Event-Driven Signal Processing with Neuromorphic Computing Systems, https://ieeexplore.ieee.org/document/9053043/\n",
    "\n",
    "print(\"Energy evaluation on Movidius: \"+str(np.round(energy*1e6,2))+\" μJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./output/tmp_{}_{}_{}/best_test/best_test_{}\".format(net_type,optim_nni_experiment,datafile[5:],optim_nni_experiment))\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=minibatch_train)\n",
    "print(\"Test accuracy: \"+str(np.round(acc*100,2))+\"%\")\n",
    "\n",
    "pred = model.predict(x_test, batch_size=minibatch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), pred.argmax(axis=1), normalize='true')\n",
    "labels = ConfusionMatrix_labels(subset)\n",
    "cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "plt.figure(figsize=(7,5.25))\n",
    "sn.heatmap(cm_df,\n",
    "           annot=True,\n",
    "           fmt='.2g',\n",
    "           cbar=False,\n",
    "           square=False,\n",
    "           cmap=\"YlGnBu\")\n",
    "plt.xlabel('\\nPredicted')\n",
    "plt.ylabel('True\\n')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('LSTM\\n', fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "if save:\n",
    "    plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-international",
   "metadata": {},
   "source": [
    "<a id='Section_2'></a>\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'cnn'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN_{}_subset{}_{}\".format(device,subset,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['nni_network/batch_size/randint']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = y_train_oh\n",
    "y_val = y_val_oh\n",
    "y_test = y_test_oh\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# First convolutional layer\n",
    "model.add(Conv1D(filters=network_parameters['nni_network/Conv1D_filters_1/randint'], \n",
    "                 kernel_size=network_parameters['nni_network/Conv1D_kernel_size_1/randint'], \n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 input_shape=(timesteps,input_dim))\n",
    "         )\n",
    "# Second convolutional layer\n",
    "model.add(Conv1D(filters=network_parameters['nni_network/Conv1D_filters_2/randint'], \n",
    "                 kernel_size=network_parameters['nni_network/Conv1D_kernel_size_2/randint'], \n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_uniform')\n",
    "         )\n",
    "# Adding a pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Adding a flattening layer\n",
    "model.add(Flatten())\n",
    "# Adding a dense layer\n",
    "model.add(Dense(network_parameters['nni_network/CNN_Dense_1/randint'], \n",
    "                activation='relu')\n",
    "         )\n",
    "# Adding a dense output layer\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "optim = Adam(lr=network_parameters['nni_network/lr/quniform'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fp, total, missed = memory_footprint(model, nengo=False)\n",
    "\n",
    "print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "print('Total:',total)\n",
    "print('Missed:',missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FLOPs: {}\".format(get_flops(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = get_flops(model)*7.53e-10 # Event-Driven Signal Processing with Neuromorphic Computing Systems, https://ieeexplore.ieee.org/document/9053043/\n",
    "\n",
    "print(\"Energy evaluation on Movidius: \"+str(np.round(energy*1e6,2))+\" μJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./output/tmp_{}_{}_{}/best_test/best_test_{}\".format(net_type,optim_nni_experiment,datafile[5:],optim_nni_experiment))\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=minibatch_train)\n",
    "print(\"Test accuracy: \"+str(np.round(acc*100,2))+\"%\")\n",
    "\n",
    "pred = model.predict(x_test, batch_size=minibatch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), pred.argmax(axis=1), normalize='true')\n",
    "labels = ConfusionMatrix_labels(subset)\n",
    "cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "plt.figure(figsize=(7,5.25))\n",
    "sn.heatmap(cm_df,\n",
    "           annot=True,\n",
    "           fmt='.2g',\n",
    "           cbar=False,\n",
    "           square=False,\n",
    "           cmap=\"YlGnBu\")\n",
    "plt.xlabel('\\nPredicted')\n",
    "plt.ylabel('True\\n')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('CNN\\n', fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "if save:\n",
    "    plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-thomson",
   "metadata": {},
   "source": [
    "<a id='Section_3'></a>\n",
    "### Spiking CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = 'cnn'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sCNN_{}_subset{}_{}\".format(device,subset,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS from NNI-optimized non-spiking CNN #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['nni_network/batch_size/randint']\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##### GET SNN PARAMETERS #####\n",
    "snn_nni_experiment = ''\n",
    "snn_nni_trial = '' \n",
    "snn_filename = 'parameter.cfg'\n",
    "snn_nni_ref = 'nni-experiments/'+snn_nni_experiment+'/trials/'+snn_nni_trial\n",
    "snn_nni_dir = os.path.expanduser('~')\n",
    "snn_filepath = os.path.join(snn_nni_dir,snn_nni_ref,snn_filename)\n",
    "\n",
    "with open(snn_filepath, 'r') as f:\n",
    "    snn_data = f.read()\n",
    "\n",
    "snn_params = json.loads(snn_data)\n",
    "snn_parameters = snn_params['parameters']\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "### flatten data and add time dimension:\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, -1))\n",
    "y_train = y_train[:,None,None]\n",
    "x_val = x_val.reshape((x_val.shape[0], 1, -1))\n",
    "y_val = y_val[:,None,None]\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, -1))\n",
    "y_test = y_test[:,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nonspiking = Sequential()    \n",
    "model_nonspiking.add(Conv1D(filters=network_parameters['nni_network/Conv1D_filters_1/randint'], kernel_size=network_parameters['nni_network/Conv1D_kernel_size_1/randint'], activation=tf.nn.relu, kernel_initializer='he_uniform', input_shape=(timesteps,input_dim), name='Conv1D_1'))\n",
    "model_nonspiking.add(Conv1D(filters=network_parameters['nni_network/Conv1D_filters_2/randint'], kernel_size=network_parameters['nni_network/Conv1D_kernel_size_2/randint'], activation=tf.nn.relu, kernel_initializer='he_uniform', name='Conv1D_2'))\n",
    "model_nonspiking.add(MaxPooling1D(pool_size=2, name='MaxPooling1D'))\n",
    "model_nonspiking.add(Flatten())\n",
    "model_nonspiking.add(Dense(network_parameters['nni_network/CNN_Dense_1/randint'], activation=tf.nn.relu, name='Dense_1'))\n",
    "model_nonspiking.add(Dense(n_classes, activation='softmax', name='Dense_2'))\n",
    "    \n",
    "### LOAD PRE-TRAINED WEIGHTS from NNI-optimized non-spiking CNN ###\n",
    "model_nonspiking.load_weights(\"./output/tmp_cnn_{}_{}/best_test/best_test_{}\".format(net_type,optim_nni_experiment,datafile[5:],optim_nni_experiment))\n",
    "\n",
    "### sequential to functional model\n",
    "input_layer = Input(batch_shape=model_nonspiking.layers[0].input_shape, name='Input')\n",
    "prev_layer = input_layer\n",
    "for num,el in enumerate(model_nonspiking.layers):\n",
    "    prev_layer = el(prev_layer)\n",
    "\n",
    "model = Model([input_layer], [prev_layer])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_layers = list(model.layers[ii].name for ii in range(len(model.layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_converter = nengo_dl.Converter(model,\n",
    "                                       max_to_avg_pool=True,\n",
    "                                       swap_activations={tf.nn.relu: nengo.SpikingRectifiedLinear()},\n",
    "                                       scale_firing_rates=snn_parameters['nni_keras2snn_network/scale_firing_rates/randint'],\n",
    "                                       synapse=snn_parameters['nni_keras2snn_network/synapse/quniform'],\n",
    "                                       )\n",
    "\n",
    "print('\\n##### neuron type now is:')\n",
    "for ii in range(len(trained_converter.net.ensembles)):\n",
    "    print('In ensemble',ii,':',trained_converter.net.ensembles[ii].neuron_type)\n",
    "print('#########################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trained_converter.net:\n",
    "    output_p = trained_converter.outputs[model.output]\n",
    "    conv0_p = nengo.Probe(trained_converter.layers[model.layers[1].get_output_at(-1)])\n",
    "    conv1_p = nengo.Probe(trained_converter.layers[model.layers[2].get_output_at(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = snn_parameters['nni_keras2snn_network/n_steps/randint']\n",
    "\n",
    "tiled_x_test = np.tile(x_test, (1, n_steps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(trained_converter.net, minibatch_size=snn_parameters['nni_keras2snn_network/batch_size/randint']) as sim:\n",
    "    snn_model_summary = sim.keras_model\n",
    "    snn_params = sum(np.prod(s.shape) for s in snn_model_summary.weights)\n",
    "    snn_trainable_params = sum(np.prod(w.shape) for w in snn_model_summary.trainable_weights)\n",
    "    print('\\n=================================================================')\n",
    "    print('Total params:','{:,d}'.format(snn_params))\n",
    "    print('Trainable params:','{:,d}'.format(snn_trainable_params))\n",
    "    print('Non-trainable params:','{:,d}'.format(snn_params-snn_trainable_params))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    mem_fp, total, missed = memory_footprint(sim)\n",
    "\n",
    "    print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "    print('Total:',total)\n",
    "    print('Missed:',missed)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sim.compile(\n",
    "                optimizer=tf.optimizers.Adam(snn_parameters['nni_keras2snn_network/lr/quniform']),\n",
    "                loss={\n",
    "                      output_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      conv0_p: tf.losses.mse,\n",
    "                      conv1_p: tf.losses.mse,\n",
    "                     },\n",
    "                loss_weights={\n",
    "                              output_p: 1, \n",
    "                              conv0_p: snn_parameters['nni_keras2snn_network/reg_conv0/quniform'], \n",
    "                              conv1_p: snn_parameters['nni_keras2snn_network/reg_conv1/quniform']\n",
    "                             },\n",
    "                metrics=[\"accuracy\"],\n",
    "               )\n",
    "    \n",
    "    sim.load_params(\"./output/tmp_s{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "        \n",
    "    data = sim.predict({trained_converter.inputs[model.input]: tiled_x_test})\n",
    "    predictions = np.argmax(data[trained_converter.outputs[model.output]][:, -1], axis=-1)\n",
    "    test_accuracy = (predictions[:] == y_test[:predictions.shape[0], 0, 0]).mean()\n",
    "    print(\"Test accuracy: \"+str(np.round(test_accuracy*100,2))+\"%\")\n",
    "    \n",
    "    save = True\n",
    "\n",
    "    cm = confusion_matrix(y_test[:np.min([len(y_test), len(predictions)]),-1,-1], predictions[:np.min([len(y_test), len(predictions)])], normalize='true')\n",
    "    labels = ConfusionMatrix_labels(subset)\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(7,5.25))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.2g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title('Spiking CNN\\n', fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('./pictures/'+model_name+'_'+snn_nni_experiment+'_'+snn_nni_trial+'.png')\n",
    "    plt.show()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-european",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sops_spikingCNN(trained_converter.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-martial",
   "metadata": {},
   "source": [
    "<a id='Section_4'></a>\n",
    "### LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'LMU'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LMU_{}_subset{}_{}\".format(device,subset_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['minibatch']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "y_train = y_train[:, None, None]\n",
    "y_test = y_test[:, None, None]\n",
    "y_val = y_val[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    # remove some unnecessary features to speed up the training\n",
    "    nengo_dl.configure_settings(\n",
    "                                trainable=None,\n",
    "                                stateful=False,\n",
    "                                keep_history=False,\n",
    "                               )\n",
    "\n",
    "    # input node\n",
    "    inp = nengo.Node(np.zeros(input_dim))\n",
    "\n",
    "    # lmu cell\n",
    "    lmu = LMUCell(\n",
    "                  units=int(network_parameters['units']), \n",
    "                  order=int(network_parameters['order']), \n",
    "                  theta=network_parameters['theta'],\n",
    "                  input_d=input_dim,\n",
    "                  tau=network_parameters['tau'],\n",
    "                )\n",
    "    conn_in = nengo.Connection(inp, lmu.x, synapse=network_parameters['synapse_in'])\n",
    "    net.config[conn_in].trainable = True\n",
    "\n",
    "    # dense linear readout\n",
    "    out = nengo.Node(size_in=n_classes)\n",
    "    conn_out = nengo.Connection(lmu.h, out, transform=nengo_dl.dists.Glorot(), synapse=network_parameters['synapse_out'])\n",
    "    net.config[conn_out].trainable = True\n",
    "\n",
    "    # record output\n",
    "    p = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_train) as sim:\n",
    "    \n",
    "    lmu_model_summary = sim.keras_model\n",
    "    lmu_params = sum(np.prod(s.shape) for s in lmu_model_summary.weights)\n",
    "    lmu_trainable_params = sum(np.prod(w.shape) for w in lmu_model_summary.trainable_weights)\n",
    "    mem_fp, total, missed = memory_footprint(sim) \n",
    "    print('\\n=================================================================')\n",
    "    print('Total params:','{:,d}'.format(lmu_params))\n",
    "    print('Trainable params:','{:,d}'.format(lmu_trainable_params))\n",
    "    print('Non-trainable params:','{:,d}'.format(lmu_params-lmu_trainable_params))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    mem_fp, total, missed = memory_footprint(sim)\n",
    "\n",
    "    print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "    print('Total:',total)\n",
    "    print('Missed:',missed)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sim.compile(\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.optimizers.Adam(network_parameters['lr']),\n",
    "                metrics=[\"accuracy\"],\n",
    "               )\n",
    "    \n",
    "    sim.load_params(\"./output/tmp_{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "    \n",
    "    test = sim.evaluate(x_test, y_test)[\"probe_accuracy\"]\n",
    "    print(\"Test accuracy: \"+str(np.round(test*100,2))+\"%\")\n",
    "    \n",
    "    prediction = sim.predict(x_test)\n",
    "        \n",
    "    predictions = list(prediction.values())[0]\n",
    "    pred = predictions.argmax(axis=-1)\n",
    "    \n",
    "    save = False\n",
    "    \n",
    "    cm = confusion_matrix(y_test[:np.min([len(y_test), len(pred)]),-1,-1], pred[:np.min([len(y_test), len(pred)]),-1], normalize='true')\n",
    "    labels = ConfusionMatrix_labels(subset)\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(7,5.25))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.2g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.title('LMU\\n', fontweight='bold', fontsize=16)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "    plt.show()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-soldier",
   "metadata": {},
   "source": [
    "<a id='Section_5'></a>\n",
    "### Spiking LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'slmu'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sLMU_{}_subset{}_{}\".format(device,subset_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['minibatch']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "y_train = y_train[:, None, None]\n",
    "y_test = y_test[:, None, None]\n",
    "y_val = y_val[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    # remove some unnecessary features to speed up the training\n",
    "    nengo_dl.configure_settings(\n",
    "        trainable=None,\n",
    "        stateful=False,\n",
    "        keep_history=False,\n",
    "    )\n",
    "\n",
    "    # input node\n",
    "    inp = nengo.Node(np.zeros(input_dim))\n",
    "    \n",
    "    order = int(network_parameters['order'])\n",
    "    theta = network_parameters['theta']\n",
    "    input_d = input_dim\n",
    "    tau = network_parameters['tau'] \n",
    "    \n",
    "    Q = np.arange(order, dtype=np.float64)\n",
    "    R = (2 * Q + 1)[:, None] / theta\n",
    "    j, i = np.meshgrid(Q, Q)\n",
    "    A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R \n",
    "    B = (-1.0) ** Q[:, None] * R \n",
    "    C = np.ones((1, order))\n",
    "    D = np.zeros((1,))\n",
    "\n",
    "    disc_step = 1/theta\n",
    "    A, B, _, _, _ = cont2discrete((A, B, C, D), dt=disc_step, method=\"zoh\")\n",
    "    \n",
    "    A_H = 1/(1-np.exp(-disc_step/tau)) * (A - np.exp(-disc_step/tau)*np.identity(order))\n",
    "    B_H = 1/(1-np.exp(-disc_step/tau)) * B\n",
    "\n",
    "    for conn in net.all_connections:\n",
    "        conn.synapse = network_parameters['synapse_all']\n",
    "\n",
    "    max_rate = network_parameters['max_rate']\n",
    "    amplitude = 1/max_rate\n",
    "    lmu_inner = nengo.networks.EnsembleArray(n_neurons=int(network_parameters['n_neurons']),\n",
    "                                             n_ensembles=order, \n",
    "                                             neuron_type=nengo.SpikingRectifiedLinear(amplitude=amplitude),\n",
    "                                             max_rates=nengo.dists.Choice([max_rate]))\n",
    "    conn_inner = nengo.Connection(lmu_inner.output, lmu_inner.input, transform=A_H, synapse=tau)\n",
    "    net.config[conn_inner].trainable = True\n",
    "    \n",
    "    conn_in = nengo.Connection(inp, lmu_inner.input, transform=np.ones((1, input_d))*B_H, synapse=network_parameters['synapse_in'])\n",
    "    net.config[conn_in].trainable = True\n",
    "    \n",
    "    # dense linear readout\n",
    "    out = nengo.Node(size_in=n_classes)\n",
    "    conn_out = nengo.Connection(lmu_inner.output, out, transform=nengo_dl.dists.Glorot(), synapse=network_parameters['synapse_out'])\n",
    "    net.config[conn_out].trainable = True\n",
    "\n",
    "    # record output\n",
    "    p = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_train) as sim: \n",
    "    \n",
    "    lmuEns_model_summary = sim.keras_model\n",
    "    lmuEns_params = sum(np.prod(s.shape) for s in lmuEns_model_summary.weights)\n",
    "    lmuEns_trainable_params = sum(np.prod(w.shape) for w in lmuEns_model_summary.trainable_weights)\n",
    "    mem_fp, total, missed = memory_footprint(sim) \n",
    "    print('\\n=================================================================')\n",
    "    print('Total params:','{:,d}'.format(lmuEns_params))\n",
    "    print('Trainable params:','{:,d}'.format(lmuEns_trainable_params))\n",
    "    print('Non-trainable params:','{:,d}'.format(lmuEns_params-lmuEns_trainable_params))\n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    mem_fp, total, missed = memory_footprint(sim)\n",
    "\n",
    "    print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "    print('Total:',total)\n",
    "    print('Missed:',missed)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sim.compile(\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.optimizers.Adam(network_parameters['lr']),\n",
    "                metrics=[\"accuracy\"],\n",
    "               )\n",
    "    \n",
    "    sim.load_params(\"./output/tmp_s{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "    \n",
    "    test = sim.evaluate(x_test, y_test)[\"probe_accuracy\"]\n",
    "    print(\"Test accuracy: \"+str(np.round(test*100,2))+\"%\")\n",
    "    \n",
    "    prediction = sim.predict(x_test)\n",
    "        \n",
    "    predictions = list(prediction.values())[0]\n",
    "    pred = predictions.argmax(axis=-1)\n",
    "    \n",
    "    save = False\n",
    "    \n",
    "    cm = confusion_matrix(y_test[:np.min([len(y_test), len(pred)]),-1,-1], pred[:np.min([len(y_test), len(pred)]),-1], normalize='true')\n",
    "    labels = ConfusionMatrix_labels(subset)\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(7,5.25))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.2g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.title('Spiking LMU\\n', fontweight='bold', fontsize=16)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "    plt.show()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-flesh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sops_LMUens(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-lounge",
   "metadata": {},
   "source": [
    "<a id='Section_6'></a>\n",
    "### LMU (ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'LMU'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LMU_freqdec_{}_subset{}_{}\".format(device,subset_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['minibatch']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dec = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "\n",
    "if freq_dec:\n",
    "    x_train = frequency_decomposition(x_train)\n",
    "    x_val = frequency_decomposition(x_val)\n",
    "    x_test = frequency_decomposition(x_test)\n",
    "\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "y_train = y_train[:, None, None]\n",
    "y_test = y_test[:, None, None]\n",
    "y_val = y_val[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    # remove some unnecessary features to speed up the training\n",
    "    nengo_dl.configure_settings(\n",
    "                                trainable=None,\n",
    "                                stateful=False,\n",
    "                                keep_history=False,\n",
    "                               )\n",
    "\n",
    "    # input node\n",
    "    inp = nengo.Node(np.zeros(input_dim))\n",
    "\n",
    "    # lmu cell\n",
    "    lmu = LMUCell(\n",
    "                  units=int(network_parameters['units']),\n",
    "                  order=int(network_parameters['order']), \n",
    "                  theta=network_parameters['theta'], \n",
    "                  input_d=input_dim,\n",
    "                  tau=network_parameters['tau'],\n",
    "                )\n",
    "    conn_in = nengo.Connection(inp, lmu.x, synapse=network_parameters['synapse_in'])\n",
    "    net.config[conn_in].trainable = True\n",
    "\n",
    "    # dense linear readout\n",
    "    out = nengo.Node(size_in=n_classes)\n",
    "    conn_out = nengo.Connection(lmu.h, out, transform=nengo_dl.dists.Glorot(), synapse=network_parameters['synapse_out'])\n",
    "    net.config[conn_out].trainable = True\n",
    "\n",
    "    # record output\n",
    "    p = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_train) as sim:\n",
    "    \n",
    "    lmu_model_summary = sim.keras_model\n",
    "    lmu_params = sum(np.prod(s.shape) for s in lmu_model_summary.weights)\n",
    "    lmu_trainable_params = sum(np.prod(w.shape) for w in lmu_model_summary.trainable_weights)\n",
    "    mem_fp, total, missed = memory_footprint(sim) \n",
    "    print('\\n=================================================================')\n",
    "    print('Total params:','{:,d}'.format(lmu_params))\n",
    "    print('Trainable params:','{:,d}'.format(lmu_trainable_params))\n",
    "    print('Non-trainable params:','{:,d}'.format(lmu_params-lmu_trainable_params)) \n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    mem_fp, total, missed = memory_footprint(sim)\n",
    "\n",
    "    print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "    print('Total:',total)\n",
    "    print('Missed:',missed)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sim.compile(\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.optimizers.Adam(network_parameters['lr']),\n",
    "                metrics=[\"accuracy\"],\n",
    "               )\n",
    "    \n",
    "    sim.load_params(\"./output/tmp_{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "    \n",
    "    test = sim.evaluate(x_test, y_test)[\"probe_accuracy\"]\n",
    "    print(\"Test accuracy: \"+str(np.round(test*100,2))+\"%\")\n",
    "    \n",
    "    prediction = sim.predict(x_test)\n",
    "        \n",
    "    predictions = list(prediction.values())[0]\n",
    "    pred = predictions.argmax(axis=-1)\n",
    "    \n",
    "    save = True\n",
    "    \n",
    "    cm = confusion_matrix(y_test[:np.min([len(y_test), len(pred)]),-1,-1], pred[:np.min([len(y_test), len(pred)]),-1], normalize='true')\n",
    "    labels = ConfusionMatrix_labels(subset)\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(7,5.25))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.2g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.title('LMU (frequency filtering)\\n', fontweight='bold', fontsize=16)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "    plt.show()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-mills",
   "metadata": {},
   "source": [
    "<a id='Section_7'></a>\n",
    "### Spiking LMU (ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'slmu'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sLMU_freqdec_{}_subset{}_{}\".format(device,subset_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['minibatch']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dec = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "\n",
    "if freq_dec:\n",
    "    x_train = frequency_decomposition(x_train)\n",
    "    x_val = frequency_decomposition(x_val)\n",
    "    x_test = frequency_decomposition(x_test)\n",
    "\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "y_train = y_train[:, None, None]\n",
    "y_test = y_test[:, None, None]\n",
    "y_val = y_val[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    # remove some unnecessary features to speed up the training\n",
    "    nengo_dl.configure_settings(\n",
    "        trainable=None,\n",
    "        stateful=False,\n",
    "        keep_history=False,\n",
    "    )\n",
    "\n",
    "    # input node\n",
    "    inp = nengo.Node(np.zeros(input_dim))\n",
    "    \n",
    "    order = int(network_parameters['order'])\n",
    "    theta = network_parameters['theta']\n",
    "    input_d = input_dim\n",
    "    tau = network_parameters['tau'] \n",
    "    \n",
    "    Q = np.arange(order, dtype=np.float64)\n",
    "    R = (2 * Q + 1)[:, None] / theta\n",
    "    j, i = np.meshgrid(Q, Q)\n",
    "    A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R \n",
    "    B = (-1.0) ** Q[:, None] * R \n",
    "    C = np.ones((1, order))\n",
    "    D = np.zeros((1,))\n",
    "\n",
    "    disc_step = 1/theta\n",
    "    A, B, _, _, _ = cont2discrete((A, B, C, D), dt=disc_step, method=\"zoh\") \n",
    "    \n",
    "    A_H = 1/(1-np.exp(-disc_step/tau)) * (A - np.exp(-disc_step/tau)*np.identity(order))\n",
    "    B_H = 1/(1-np.exp(-disc_step/tau)) * B\n",
    "\n",
    "    for conn in net.all_connections:\n",
    "        conn.synapse = network_parameters['synapse_all']\n",
    "\n",
    "    max_rate = network_parameters['max_rate']\n",
    "    amplitude = 1/max_rate\n",
    "    lmu_inner = nengo.networks.EnsembleArray(n_neurons=int(network_parameters['n_neurons']),\n",
    "                                             n_ensembles=order, \n",
    "                                             neuron_type=nengo.SpikingRectifiedLinear(amplitude=amplitude),\n",
    "                                             max_rates=nengo.dists.Choice([max_rate]))\n",
    "    conn_inner = nengo.Connection(lmu_inner.output, lmu_inner.input, transform=A_H, synapse=tau)\n",
    "    net.config[conn_inner].trainable = True\n",
    "    \n",
    "    conn_in = nengo.Connection(inp, lmu_inner.input, transform=np.ones((1, input_d))*B_H, synapse=network_parameters['synapse_in'])\n",
    "    net.config[conn_in].trainable = True\n",
    "    \n",
    "    # dense linear readout\n",
    "    out = nengo.Node(size_in=n_classes)\n",
    "    conn_out = nengo.Connection(lmu_inner.output, out, transform=nengo_dl.dists.Glorot(), synapse=network_parameters['synapse_out'])\n",
    "    net.config[conn_out].trainable = True\n",
    "\n",
    "    # record output\n",
    "    p = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_train) as sim: \n",
    "    \n",
    "    lmuEns_model_summary = sim.keras_model\n",
    "    lmuEns_params = sum(np.prod(s.shape) for s in lmuEns_model_summary.weights)\n",
    "    lmuEns_trainable_params = sum(np.prod(w.shape) for w in lmuEns_model_summary.trainable_weights)\n",
    "    mem_fp, total, missed = memory_footprint(sim) \n",
    "    print('\\n=================================================================')\n",
    "    print('Total params:','{:,d}'.format(lmuEns_params))\n",
    "    print('Trainable params:','{:,d}'.format(lmuEns_trainable_params))\n",
    "    print('Non-trainable params:','{:,d}'.format(lmuEns_params-lmuEns_trainable_params)) \n",
    "    print('_________________________________________________________________\\n')\n",
    "    \n",
    "    mem_fp, total, missed = memory_footprint(sim)\n",
    "\n",
    "    print('Memory footprint (MB):',np.round(mem_fp,4))\n",
    "    print('Total:',total)\n",
    "    print('Missed:',missed)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sim.compile(\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.optimizers.Adam(network_parameters['lr']),\n",
    "                metrics=[\"accuracy\"],\n",
    "               )\n",
    "    \n",
    "    sim.load_params(\"./output/tmp_s{}_{}_{}/best_test_{}\".format(net_type,snn_nni_experiment,datafile[5:],snn_nni_experiment))\n",
    "    \n",
    "    test = sim.evaluate(x_test, y_test)[\"probe_accuracy\"]\n",
    "    print(\"Test accuracy: \"+str(np.round(test*100,2))+\"%\")\n",
    "    \n",
    "    prediction = sim.predict(x_test)\n",
    "        \n",
    "    predictions = list(prediction.values())[0]\n",
    "    pred = predictions.argmax(axis=-1)\n",
    "    \n",
    "    save = True\n",
    "    \n",
    "    cm = confusion_matrix(y_test[:np.min([len(y_test), len(pred)]),-1,-1], pred[:np.min([len(y_test), len(pred)]),-1], normalize='true')\n",
    "    labels = ConfusionMatrix_labels(subset)\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[jj for jj in labels])\n",
    "    plt.figure(figsize=(7,5.25))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.2g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.title('Spiking LMU (frequency filtering)\\n', fontweight='bold', fontsize=16)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('./pictures/'+model_name+'_'+optim_nni_experiment+'_'+optim_nni_trial+'.png')\n",
    "    plt.show()\n",
    "\n",
    "sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-romantic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sops_LMUens(net, freq_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792081e0",
   "metadata": {},
   "source": [
    "<a id='Section_8'></a>\n",
    "### <i>FLOPs calculation and energy estimation for LMU and LMU (ff)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core classes for the KerasLMU package.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(tf.__version__) < version.parse(\"2.6.0rc0\"):\n",
    "    from tensorflow.python.keras.layers.recurrent import DropoutRNNCellMixin\n",
    "else:\n",
    "    from keras.layers.recurrent import DropoutRNNCellMixin\n",
    "\n",
    "\n",
    "class LMUCell(DropoutRNNCellMixin, tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of LMU cell (to be used within Keras RNN wrapper).\n",
    "    In general, the LMU cell consists of two parts: a memory component (decomposing\n",
    "    the input signal using Legendre polynomials as a basis), and a hidden component\n",
    "    (learning nonlinear mappings from the memory component). [1]_ [2]_\n",
    "    This class processes one step within the whole time sequence input. Use the ``LMU``\n",
    "    class to create a recurrent Keras layer to process the whole sequence. Calling\n",
    "    ``LMU()`` is equivalent to doing ``RNN(LMUCell())``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_d : int\n",
    "        Dimensionality of input to memory component.\n",
    "    order : int\n",
    "        The number of degrees in the transfer function of the LTI system used to\n",
    "        represent the sliding window of history. This parameter sets the number of\n",
    "        Legendre polynomials used to orthogonally represent the sliding window.\n",
    "    theta : float\n",
    "        The number of timesteps in the sliding window that is represented using the\n",
    "        LTI system. In this context, the sliding window represents a dynamic range of\n",
    "        data, of fixed size, that will be used to predict the value at the next time\n",
    "        step. If this value is smaller than the size of the input sequence, only that\n",
    "        number of steps will be represented at the time of prediction, however the\n",
    "        entire sequence will still be processed in order for information to be\n",
    "        projected to and from the hidden layer. If ``trainable_theta`` is enabled, then\n",
    "        theta will be updated during the course of training.\n",
    "    hidden_cell : ``tf.keras.layers.Layer``\n",
    "        Keras Layer/RNNCell implementing the hidden component.\n",
    "    trainable_theta : bool\n",
    "        If True, theta is learnt over the course of training. Otherwise, it is kept\n",
    "        constant.\n",
    "    hidden_to_memory : bool\n",
    "        If True, connect the output of the hidden component back to the memory\n",
    "        component (default False).\n",
    "    memory_to_memory : bool\n",
    "        If True, add a learnable recurrent connection (in addition to the static\n",
    "        Legendre system) to the memory component (default False).\n",
    "    input_to_hidden : bool\n",
    "        If True, connect the input directly to the hidden component (in addition to\n",
    "        the connection from the memory component) (default False).\n",
    "    discretizer : str\n",
    "        The method used to discretize the A and B matrices of the LMU. Current\n",
    "        options are \"zoh\" (short for Zero Order Hold) and \"euler\".\n",
    "        \"zoh\" is more accurate, but training will be slower than \"euler\" if\n",
    "        ``trainable_theta=True``. Note that a larger theta is needed when discretizing\n",
    "        using \"euler\" (a value that is larger than ``4*order`` is recommended).\n",
    "    kernel_initializer : ``tf.initializers.Initializer``\n",
    "        Initializer for weights from input to memory/hidden component. If ``None``,\n",
    "        no weights will be used, and the input size must match the memory/hidden size.\n",
    "    recurrent_initializer : ``tf.initializers.Initializer``\n",
    "        Initializer for ``memory_to_memory`` weights (if that connection is enabled).\n",
    "    dropout : float\n",
    "        Dropout rate on input connections.\n",
    "    recurrent_dropout : float\n",
    "        Dropout rate on ``memory_to_memory`` connection.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Voelker and Eliasmith (2018). Improving spiking dynamical\n",
    "       networks: Accurate delays, higher-order synapses, and time cells.\n",
    "       Neural Computation, 30(3): 569-609.\n",
    "    .. [2] Voelker and Eliasmith. \"Methods and systems for implementing\n",
    "       dynamic neural networks.\" U.S. Patent Application No. 15/243,223.\n",
    "       Filing date: 2016-08-22.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        memory_d,\n",
    "        order,\n",
    "        theta,\n",
    "        hidden_cell,\n",
    "        trainable_theta=False,\n",
    "        hidden_to_memory=False,\n",
    "        memory_to_memory=False,\n",
    "        input_to_hidden=False,\n",
    "        discretizer=\"zoh\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        dropout=0,\n",
    "        recurrent_dropout=0,\n",
    "        tau=0.001, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.memory_d = memory_d\n",
    "        self.order = order\n",
    "        self._init_theta = theta\n",
    "        self.hidden_cell = hidden_cell\n",
    "        self.trainable_theta = trainable_theta\n",
    "        self.hidden_to_memory = hidden_to_memory\n",
    "        self.memory_to_memory = memory_to_memory\n",
    "        self.input_to_hidden = input_to_hidden\n",
    "        self.discretizer = discretizer\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.recurrent_initializer = recurrent_initializer\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.tau = tau\n",
    "\n",
    "        self.kernel = None\n",
    "        self.recurrent_kernel = None\n",
    "        self.theta_inv = None\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "        if self.discretizer not in (\"zoh\", \"euler\"):\n",
    "            raise ValueError(\n",
    "                f\"discretizer must be 'zoh' or 'euler' (got '{self.discretizer}')\"\n",
    "            )\n",
    "\n",
    "        if self.hidden_cell is None:\n",
    "            for conn in (\"hidden_to_memory\", \"input_to_hidden\"):\n",
    "                if getattr(self, conn):\n",
    "                    raise ValueError(f\"{conn} must be False if hidden_cell is None\")\n",
    "\n",
    "            self.hidden_output_size = self.memory_d * self.order\n",
    "            self.hidden_state_size = []\n",
    "        elif hasattr(self.hidden_cell, \"state_size\"):\n",
    "            self.hidden_output_size = self.hidden_cell.output_size\n",
    "            self.hidden_state_size = self.hidden_cell.state_size\n",
    "        else:\n",
    "            # TODO: support layers that don't have the `units` attribute\n",
    "            self.hidden_output_size = self.hidden_cell.units\n",
    "            self.hidden_state_size = [self.hidden_cell.units]\n",
    "\n",
    "        self.state_size = tf.nest.flatten(self.hidden_state_size) + [\n",
    "            self.memory_d * self.order\n",
    "        ]\n",
    "        self.output_size = self.hidden_output_size\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        \"\"\"\n",
    "        Value of the ``theta`` parameter.\n",
    "        If ``trainable_theta=True`` this returns the trained value, not the initial\n",
    "        value passed in to the constructor.\n",
    "        \"\"\"\n",
    "        if self.built:\n",
    "            return 1 / tf.keras.backend.get_value(self.theta_inv)\n",
    "\n",
    "        return self._init_theta\n",
    "\n",
    "    def _gen_AB(self):\n",
    "        \"\"\"Generates A and B matrices.\"\"\"\n",
    "\n",
    "        # compute analog A/B matrices\n",
    "        Q = np.arange(self.order, dtype=np.float64)\n",
    "        R = (2 * Q + 1)[:, None] / self._init_theta\n",
    "        j, i = np.meshgrid(Q, Q)\n",
    "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
    "        B = (-1.0) ** Q[:, None] * R\n",
    "        \n",
    "        disc_step = 1/self._init_theta\n",
    "        A = 1/(1-np.exp(-disc_step/self.tau)) * (A - np.exp(-disc_step/self.tau)*np.identity(self.order))  \n",
    "        B = 1/(1-np.exp(-disc_step/self.tau)) * B  \n",
    "\n",
    "        # discretize matrices\n",
    "        if self.discretizer == \"zoh\":\n",
    "            # save the un-discretized matrices for use in .call\n",
    "            self._base_A = tf.constant(A.T, dtype=self.dtype)\n",
    "            self._base_B = tf.constant(B.T, dtype=self.dtype)\n",
    "            \n",
    "            disc_step = 1/self._init_theta\n",
    "            self.A, self.B = LMUCell._cont2discrete_zoh(\n",
    "                self._base_A / self._init_theta, self._base_B / self._init_theta, disc_step\n",
    "            )\n",
    "        else:\n",
    "            if not self.trainable_theta:\n",
    "                A = A / self._init_theta + np.eye(self.order)\n",
    "                B = B / self._init_theta\n",
    "\n",
    "            self.A = tf.constant(A.T, dtype=self.dtype)\n",
    "            self.B = tf.constant(B.T, dtype=self.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cont2discrete_zoh(A, B, dt):\n",
    "        \"\"\"\n",
    "        Function to discretize A and B matrices using Zero Order Hold method.\n",
    "        Functionally equivalent to\n",
    "        ``scipy.signal.cont2discrete((A.T, B.T, _, _), method=\"zoh\", dt=1.0)``\n",
    "        (but implemented in TensorFlow so that it is differentiable).\n",
    "        Note that this accepts and returns matrices that are transposed from the\n",
    "        standard linear system implementation (as that makes it easier to use in\n",
    "        `.call`).\n",
    "        \"\"\"\n",
    "\n",
    "        # combine A/B and pad to make square matrix\n",
    "        em_upper = tf.concat([A, B], axis=0)\n",
    "        em = tf.pad(em_upper, [(0, 0), (0, B.shape[0])])\n",
    "\n",
    "        # compute matrix exponential\n",
    "        ms = tf.linalg.expm(dt*em)\n",
    "\n",
    "        # slice A/B back out of combined matrix\n",
    "        discrt_A = ms[: A.shape[0], : A.shape[1]]\n",
    "        discrt_B = ms[A.shape[0] :, : A.shape[1]]\n",
    "\n",
    "        return discrt_A, discrt_B\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Builds the cell.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "        enc_d = input_shape[-1]\n",
    "        if self.hidden_to_memory:\n",
    "            enc_d += self.hidden_output_size\n",
    "\n",
    "        if self.kernel_initializer is not None:\n",
    "            self.kernel = self.add_weight(\n",
    "                name=\"kernel\",\n",
    "                shape=(enc_d, self.memory_d),\n",
    "                initializer=self.kernel_initializer,\n",
    "            )\n",
    "        else:\n",
    "            self.kernel = None\n",
    "            if enc_d != self.memory_d:\n",
    "                raise ValueError(\n",
    "                    f\"For LMUCells with no input kernel, the input dimension ({enc_d})\"\n",
    "                    f\" must equal `memory_d` ({self.memory_d}).\"\n",
    "                )\n",
    "\n",
    "        # when using euler, 1/theta results in better gradients for the memory\n",
    "        # update since you are multiplying 1/theta, as compared to dividing theta\n",
    "        if self.trainable_theta:\n",
    "            self.theta_inv = self.add_weight(\n",
    "                name=\"theta_inv\",\n",
    "                shape=(),\n",
    "                initializer=tf.initializers.constant(1 / self._init_theta),\n",
    "                constraint=tf.keras.constraints.NonNeg(),\n",
    "            )\n",
    "        else:\n",
    "            self.theta_inv = tf.constant(1 / self._init_theta, dtype=self.dtype)\n",
    "\n",
    "        if self.memory_to_memory:\n",
    "            self.recurrent_kernel = self.add_weight(\n",
    "                name=\"recurrent_kernel\",\n",
    "                shape=(self.memory_d * self.order, self.memory_d),\n",
    "                initializer=self.recurrent_initializer,\n",
    "            )\n",
    "        else:\n",
    "            self.recurrent_kernel = None\n",
    "\n",
    "        if self.hidden_cell is not None and not self.hidden_cell.built:\n",
    "            hidden_input_d = self.memory_d * self.order\n",
    "            if self.input_to_hidden:\n",
    "                hidden_input_d += input_shape[-1]\n",
    "            with tf.name_scope(self.hidden_cell.name):\n",
    "                self.hidden_cell.build((input_shape[0], hidden_input_d))\n",
    "\n",
    "        # generate A and B matrices\n",
    "        self._gen_AB()\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        \"\"\"\n",
    "        Apply this cell to inputs.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "\n",
    "        states = tf.nest.flatten(states)\n",
    "\n",
    "        # state for the hidden cell\n",
    "        h = states[:-1]\n",
    "        # state for the LMU memory\n",
    "        m = states[-1]\n",
    "\n",
    "        # compute memory input\n",
    "        u_in = tf.concat((inputs, h[0]), axis=1) if self.hidden_to_memory else inputs\n",
    "        if self.dropout > 0:\n",
    "            u_in *= self.get_dropout_mask_for_cell(u_in, training)\n",
    "        u = u_in if self.kernel is None else tf.matmul(u_in, self.kernel)\n",
    "\n",
    "        if self.memory_to_memory:\n",
    "            if self.recurrent_dropout > 0:\n",
    "                # note: we don't apply dropout to the memory input, only\n",
    "                # the recurrent kernel\n",
    "                rec_m = m * self.get_recurrent_dropout_mask_for_cell(m, training)\n",
    "            else:\n",
    "                rec_m = m\n",
    "\n",
    "            u += tf.matmul(rec_m, self.recurrent_kernel)\n",
    "\n",
    "        # separate memory/order dimensions\n",
    "        m = tf.reshape(m, (-1, self.memory_d, self.order))\n",
    "        u = tf.expand_dims(u, -1)\n",
    "\n",
    "        # update memory\n",
    "        if self.discretizer == \"zoh\" and self.trainable_theta:\n",
    "            # apply updated theta and re-discretize\n",
    "            A, B = LMUCell._cont2discrete_zoh(\n",
    "                self._base_A * self.theta_inv, self._base_B * self.theta_inv\n",
    "            )\n",
    "        else:\n",
    "            A, B = self.A, self.B\n",
    "\n",
    "        _m = tf.matmul(m, A) + tf.matmul(u, B)\n",
    "\n",
    "        if self.discretizer == \"euler\" and self.trainable_theta:\n",
    "            # apply updated theta. this is the same as scaling A/B by theta, but it's\n",
    "            # more efficient to do it this way.\n",
    "            # note that when computing this way the A matrix does not\n",
    "            # include the identity matrix along the diagonal (since we don't want to\n",
    "            # scale that part by theta), which is why we do += instead of =\n",
    "            m += _m * self.theta_inv\n",
    "        else:\n",
    "            m = _m\n",
    "\n",
    "        # re-combine memory/order dimensions\n",
    "        m = tf.reshape(m, (-1, self.memory_d * self.order))\n",
    "\n",
    "        # apply hidden cell\n",
    "        h_in = tf.concat((m, inputs), axis=1) if self.input_to_hidden else m\n",
    "\n",
    "        if self.hidden_cell is None:\n",
    "            o = h_in\n",
    "            h = []\n",
    "        elif hasattr(self.hidden_cell, \"state_size\"):\n",
    "            o, h = self.hidden_cell(h_in, h, training=training)\n",
    "        else:\n",
    "            o = self.hidden_cell(h_in, training=training)\n",
    "            h = [o]\n",
    "\n",
    "        return o, h + [m]\n",
    "\n",
    "    def reset_dropout_mask(self):\n",
    "        \"\"\"Reset dropout mask for memory and hidden components.\"\"\"\n",
    "        super().reset_dropout_mask()\n",
    "        if isinstance(self.hidden_cell, DropoutRNNCellMixin):\n",
    "            self.hidden_cell.reset_dropout_mask()\n",
    "\n",
    "    def reset_recurrent_dropout_mask(self):\n",
    "        \"\"\"Reset recurrent dropout mask for memory and hidden components.\"\"\"\n",
    "        super().reset_recurrent_dropout_mask()\n",
    "        if isinstance(self.hidden_cell, DropoutRNNCellMixin):\n",
    "            self.hidden_cell.reset_recurrent_dropout_mask()\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return config of layer (for serialization during model saving/loading).\"\"\"\n",
    "\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            dict(\n",
    "                memory_d=self.memory_d,\n",
    "                order=self.order,\n",
    "                theta=self._init_theta,\n",
    "                hidden_cell=tf.keras.layers.serialize(self.hidden_cell),\n",
    "                trainable_theta=self.trainable_theta,\n",
    "                hidden_to_memory=self.hidden_to_memory,\n",
    "                memory_to_memory=self.memory_to_memory,\n",
    "                input_to_hidden=self.input_to_hidden,\n",
    "                discretizer=self.discretizer,\n",
    "                kernel_initializer=self.kernel_initializer,\n",
    "                recurrent_initializer=self.recurrent_initializer,\n",
    "                dropout=self.dropout,\n",
    "                recurrent_dropout=self.recurrent_dropout,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Load model from serialized config.\"\"\"\n",
    "\n",
    "        config[\"hidden_cell\"] = tf.keras.layers.deserialize(config[\"hidden_cell\"])\n",
    "        return super().from_config(config)\n",
    "\n",
    "\n",
    "class LMU(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A layer of trainable low-dimensional delay systems.\n",
    "    Each unit buffers its encoded input\n",
    "    by internally representing a low-dimensional\n",
    "    (i.e., compressed) version of the sliding window.\n",
    "    Nonlinear decodings of this representation,\n",
    "    expressed by the A and B matrices, provide\n",
    "    computations across the window, such as its\n",
    "    derivative, energy, median value, etc ([1]_, [2]_).\n",
    "    Note that these decoder matrices can span across\n",
    "    all of the units of an input sequence.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_d : int\n",
    "        Dimensionality of input to memory component.\n",
    "    order : int\n",
    "        The number of degrees in the transfer function of the LTI system used to\n",
    "        represent the sliding window of history. This parameter sets the number of\n",
    "        Legendre polynomials used to orthogonally represent the sliding window.\n",
    "    theta : float\n",
    "        The number of timesteps in the sliding window that is represented using the\n",
    "        LTI system. In this context, the sliding window represents a dynamic range of\n",
    "        data, of fixed size, that will be used to predict the value at the next time\n",
    "        step. If this value is smaller than the size of the input sequence, only that\n",
    "        number of steps will be represented at the time of prediction, however the\n",
    "        entire sequence will still be processed in order for information to be\n",
    "        projected to and from the hidden layer. If ``trainable_theta`` is enabled, then\n",
    "        theta will be updated during the course of training.\n",
    "    hidden_cell : ``tf.keras.layers.Layer``\n",
    "        Keras Layer/RNNCell implementing the hidden component.\n",
    "    trainable_theta : bool\n",
    "        If True, theta is learnt over the course of training. Otherwise, it is kept\n",
    "        constant.\n",
    "    hidden_to_memory : bool\n",
    "        If True, connect the output of the hidden component back to the memory\n",
    "        component (default False).\n",
    "    memory_to_memory : bool\n",
    "        If True, add a learnable recurrent connection (in addition to the static\n",
    "        Legendre system) to the memory component (default False).\n",
    "    input_to_hidden : bool\n",
    "        If True, connect the input directly to the hidden component (in addition to\n",
    "        the connection from the memory component) (default False).\n",
    "    discretizer : str\n",
    "        The method used to discretize the A and B matrices of the LMU. Current\n",
    "        options are \"zoh\" (short for Zero Order Hold) and \"euler\".\n",
    "        \"zoh\" is more accurate, but training will be slower than \"euler\" if\n",
    "        ``trainable_theta=True``. Note that a larger theta is needed when discretizing\n",
    "        using \"euler\" (a value that is larger than ``4*order`` is recommended).\n",
    "    kernel_initializer : ``tf.initializers.Initializer``\n",
    "        Initializer for weights from input to memory/hidden component. If ``None``,\n",
    "        no weights will be used, and the input size must match the memory/hidden size.\n",
    "    recurrent_initializer : ``tf.initializers.Initializer``\n",
    "        Initializer for ``memory_to_memory`` weights (if that connection is enabled).\n",
    "    dropout : float\n",
    "        Dropout rate on input connections.\n",
    "    recurrent_dropout : float\n",
    "        Dropout rate on ``memory_to_memory`` connection.\n",
    "    return_sequences : bool, optional\n",
    "        If True, return the full output sequence. Otherwise, return just the last\n",
    "        output in the output sequence.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Voelker and Eliasmith (2018). Improving spiking dynamical\n",
    "       networks: Accurate delays, higher-order synapses, and time cells.\n",
    "       Neural Computation, 30(3): 569-609.\n",
    "    .. [2] Voelker and Eliasmith. \"Methods and systems for implementing\n",
    "       dynamic neural networks.\" U.S. Patent Application No. 15/243,223.\n",
    "       Filing date: 2016-08-22.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        memory_d,\n",
    "        order,\n",
    "        theta,\n",
    "        hidden_cell,\n",
    "        trainable_theta=False,\n",
    "        hidden_to_memory=False,\n",
    "        memory_to_memory=False,\n",
    "        input_to_hidden=False,\n",
    "        discretizer=\"zoh\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        dropout=0,\n",
    "        recurrent_dropout=0,\n",
    "        return_sequences=False,\n",
    "        tau=0.001,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.memory_d = memory_d\n",
    "        self.order = order\n",
    "        self._init_theta = theta\n",
    "        self.hidden_cell = hidden_cell\n",
    "        self.trainable_theta = trainable_theta\n",
    "        self.hidden_to_memory = hidden_to_memory\n",
    "        self.memory_to_memory = memory_to_memory\n",
    "        self.input_to_hidden = input_to_hidden\n",
    "        self.discretizer = discretizer\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.recurrent_initializer = recurrent_initializer\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.return_sequences = return_sequences\n",
    "        self.layer = None\n",
    "        self.tau = tau\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        \"\"\"\n",
    "        Value of the ``theta`` parameter.\n",
    "        If ``trainable_theta=True`` this returns the trained value, not the initial\n",
    "        value passed in to the constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.built:\n",
    "            return (\n",
    "                self.layer.theta\n",
    "                if isinstance(self.layer, LMUFeedforward)\n",
    "                else self.layer.cell.theta\n",
    "            )\n",
    "\n",
    "        return self._init_theta\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"\n",
    "        Builds the layer.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        super().build(input_shapes)\n",
    "\n",
    "        if (\n",
    "            not self.hidden_to_memory\n",
    "            and not self.memory_to_memory\n",
    "            and input_shapes[1] is not None\n",
    "            and not self.trainable_theta\n",
    "        ):\n",
    "            self.layer = LMUFeedforward(\n",
    "                memory_d=self.memory_d,\n",
    "                order=self.order,\n",
    "                theta=self._init_theta,\n",
    "                hidden_cell=self.hidden_cell,\n",
    "                input_to_hidden=self.input_to_hidden,\n",
    "                discretizer=self.discretizer,\n",
    "                kernel_initializer=self.kernel_initializer,\n",
    "                dropout=self.dropout,\n",
    "                return_sequences=self.return_sequences,\n",
    "            )\n",
    "        else:\n",
    "            self.layer = tf.keras.layers.RNN(\n",
    "                LMUCell(\n",
    "                    memory_d=self.memory_d,\n",
    "                    order=self.order,\n",
    "                    theta=self._init_theta,\n",
    "                    hidden_cell=self.hidden_cell,\n",
    "                    trainable_theta=self.trainable_theta,\n",
    "                    hidden_to_memory=self.hidden_to_memory,\n",
    "                    memory_to_memory=self.memory_to_memory,\n",
    "                    input_to_hidden=self.input_to_hidden,\n",
    "                    discretizer=self.discretizer,\n",
    "                    kernel_initializer=self.kernel_initializer,\n",
    "                    recurrent_initializer=self.recurrent_initializer,\n",
    "                    dropout=self.dropout,\n",
    "                    recurrent_dropout=self.recurrent_dropout,\n",
    "                    tau=self.tau,\n",
    "                ),\n",
    "                return_sequences=self.return_sequences,\n",
    "            )\n",
    "\n",
    "        self.layer.build(input_shapes)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Apply this layer to inputs.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.layer.call(inputs, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return config of layer (for serialization during model saving/loading).\"\"\"\n",
    "\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            dict(\n",
    "                memory_d=self.memory_d,\n",
    "                order=self.order,\n",
    "                theta=self._init_theta,\n",
    "                hidden_cell=tf.keras.layers.serialize(self.hidden_cell),\n",
    "                trainable_theta=self.trainable_theta,\n",
    "                hidden_to_memory=self.hidden_to_memory,\n",
    "                memory_to_memory=self.memory_to_memory,\n",
    "                input_to_hidden=self.input_to_hidden,\n",
    "                discretizer=self.discretizer,\n",
    "                kernel_initializer=self.kernel_initializer,\n",
    "                recurrent_initializer=self.recurrent_initializer,\n",
    "                dropout=self.dropout,\n",
    "                recurrent_dropout=self.recurrent_dropout,\n",
    "                return_sequences=self.return_sequences,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Load model from serialized config.\"\"\"\n",
    "\n",
    "        config[\"hidden_cell\"] = tf.keras.layers.deserialize(config[\"hidden_cell\"])\n",
    "        return super().from_config(config)\n",
    "\n",
    "\n",
    "class LMUFeedforward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer class for the feedforward variant of the LMU.\n",
    "    This class assumes no recurrent connections are desired in the memory component.\n",
    "    Produces the output of the delay system by evaluating the convolution of the input\n",
    "    sequence with the impulse response from the LMU cell.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_d : int\n",
    "        Dimensionality of input to memory component.\n",
    "    order : int\n",
    "        The number of degrees in the transfer function of the LTI system used to\n",
    "        represent the sliding window of history. This parameter sets the number of\n",
    "        Legendre polynomials used to orthogonally represent the sliding window.\n",
    "    theta : float\n",
    "        The number of timesteps in the sliding window that is represented using the\n",
    "        LTI system. In this context, the sliding window represents a dynamic range of\n",
    "        data, of fixed size, that will be used to predict the value at the next time\n",
    "        step. If this value is smaller than the size of the input sequence, only that\n",
    "        number of steps will be represented at the time of prediction, however the\n",
    "        entire sequence will still be processed in order for information to be\n",
    "        projected to and from the hidden layer.\n",
    "    hidden_cell : ``tf.keras.layers.Layer``\n",
    "        Keras Layer implementing the hidden component.\n",
    "    input_to_hidden : bool\n",
    "        If True, connect the input directly to the hidden component (in addition to\n",
    "        the connection from the memory component) (default False).\n",
    "    discretizer : str\n",
    "        The method used to discretize the A and B matrices of the LMU. Current\n",
    "        options are \"zoh\" (short for Zero Order Hold) and \"euler\".\n",
    "        \"zoh\" is more accurate, but training will be slower than \"euler\" if\n",
    "        ``trainable_theta=True``. Note that a larger theta is needed when discretizing\n",
    "        using \"euler\" (a value that is larger than ``4*order`` is recommended).\n",
    "    kernel_initializer : ``tf.initializers.Initializer``\n",
    "        Initializer for weights from input to memory/hidden component. If ``None``,\n",
    "        no weights will be used, and the input size must match the memory/hidden size.\n",
    "    dropout : float\n",
    "        Dropout rate on input connections.\n",
    "    return_sequences : bool, optional\n",
    "        If True, return the full output sequence. Otherwise, return just the last\n",
    "        output in the output sequence.\n",
    "    conv_mode : \"fft\" or \"raw\"\n",
    "        The method for performing the inpulse response convolution. \"fft\" uses FFT\n",
    "        convolution (default). \"raw\" uses explicit convolution, which may be faster\n",
    "        for particular models on particular hardware.\n",
    "    truncate_ir : float\n",
    "        The portion of the impulse response to truncate when using \"raw\"\n",
    "        convolution (see ``conv_mode``). This is an approximate upper bound on the error\n",
    "        relative to the exact implementation. Smaller ``theta`` values result in more\n",
    "        truncated elements for a given value of ``truncate_ir``, improving efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        memory_d,\n",
    "        order,\n",
    "        theta,\n",
    "        hidden_cell,\n",
    "        input_to_hidden=False,\n",
    "        discretizer=\"zoh\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        dropout=0,\n",
    "        return_sequences=False,\n",
    "        conv_mode=\"fft\",\n",
    "        truncate_ir=1e-4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if input_to_hidden and hidden_cell is None:\n",
    "            raise ValueError(\"input_to_hidden must be False if hidden_cell is None\")\n",
    "\n",
    "        if conv_mode not in (\"fft\", \"raw\"):\n",
    "            raise ValueError(f\"Unrecognized conv mode '{conv_mode}'\")\n",
    "\n",
    "        self.memory_d = memory_d\n",
    "        self.order = order\n",
    "        self.theta = theta\n",
    "        self.hidden_cell = hidden_cell\n",
    "        self.input_to_hidden = input_to_hidden\n",
    "        self.discretizer = discretizer\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.dropout = dropout\n",
    "        self.return_sequences = return_sequences\n",
    "        self.conv_mode = conv_mode.lower()\n",
    "        self.truncate_ir = truncate_ir\n",
    "\n",
    "        # create a standard LMUCell to generate the impulse response during `build`\n",
    "        self.delay_layer = tf.keras.layers.RNN(\n",
    "            LMUCell(\n",
    "                memory_d=1,\n",
    "                order=order,\n",
    "                theta=theta,\n",
    "                hidden_cell=None,\n",
    "                trainable_theta=False,\n",
    "                input_to_hidden=False,\n",
    "                hidden_to_memory=False,\n",
    "                memory_to_memory=False,\n",
    "                discretizer=discretizer,\n",
    "                kernel_initializer=None,\n",
    "                trainable=False,\n",
    "            ),\n",
    "            return_sequences=True,\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Builds the layer.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "        seq_len = input_shape[1]\n",
    "        enc_d = input_shape[-1]\n",
    "\n",
    "        if seq_len is None:\n",
    "            # TODO: we could dynamically run the impulse response for longer if\n",
    "            #  needed using stateful=True\n",
    "            raise ValueError(\n",
    "                f\"LMUFeedforward requires that the input shape's temporal axis be \"\n",
    "                f\"fully specified (got {seq_len})\"\n",
    "            )\n",
    "\n",
    "        impulse = tf.reshape(tf.eye(seq_len, 1), (1, -1, 1))\n",
    "\n",
    "        self.impulse_response = tf.squeeze(\n",
    "            self.delay_layer(impulse, training=False), axis=0\n",
    "        )\n",
    "\n",
    "        if self.conv_mode == \"fft\":\n",
    "            self.impulse_response = tf.signal.rfft(\n",
    "                tf.transpose(self.impulse_response),\n",
    "                fft_length=[2 * seq_len],\n",
    "            )\n",
    "        else:\n",
    "            if self.truncate_ir is not None:\n",
    "                assert self.impulse_response.shape == (seq_len, self.order)\n",
    "\n",
    "                cumsum = tf.math.cumsum(\n",
    "                    tf.math.abs(self.impulse_response), axis=0, reverse=True\n",
    "                )\n",
    "                cumsum = cumsum / cumsum[0]\n",
    "                to_drop = tf.reduce_all(cumsum < self.truncate_ir, axis=-1)\n",
    "                if to_drop[-1]:\n",
    "                    cutoff = tf.where(to_drop)[0, -1]\n",
    "                    self.impulse_response = self.impulse_response[:cutoff]\n",
    "\n",
    "            self.impulse_response = tf.reshape(\n",
    "                self.impulse_response,\n",
    "                (self.impulse_response.shape[0], 1, 1, self.order),\n",
    "            )\n",
    "            self.impulse_response = self.impulse_response[::-1, :, :, :]\n",
    "\n",
    "        if self.kernel_initializer is not None:\n",
    "            self.kernel = self.add_weight(\n",
    "                name=\"kernel\",\n",
    "                shape=(input_shape[-1], self.memory_d),\n",
    "                initializer=self.kernel_initializer,\n",
    "            )\n",
    "        else:\n",
    "            self.kernel = None\n",
    "            if enc_d != self.memory_d:\n",
    "                raise ValueError(\n",
    "                    f\"For LMUCells with no input kernel, the input dimension ({enc_d})\"\n",
    "                    f\" must equal `memory_d` ({self.memory_d}).\"\n",
    "                )\n",
    "\n",
    "        if self.hidden_cell is not None and not self.hidden_cell.built:\n",
    "            hidden_input_d = self.memory_d * self.order\n",
    "            if self.input_to_hidden:\n",
    "                hidden_input_d += input_shape[-1]\n",
    "            with tf.name_scope(self.hidden_cell.name):\n",
    "                self.hidden_cell.build((input_shape[0], hidden_input_d))\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Apply this layer to inputs.\n",
    "        Notes\n",
    "        -----\n",
    "        This method should not be called manually; rather, use the implicit layer\n",
    "        callable behaviour (like ``my_layer(inputs)``), which will apply this method\n",
    "        with some additional bookkeeping.\n",
    "        \"\"\"\n",
    "\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "\n",
    "        if self.dropout:\n",
    "            inputs = tf.keras.layers.Dropout(\n",
    "                self.dropout, noise_shape=(inputs.shape[0], 1) + inputs.shape[2:]\n",
    "            )(inputs)\n",
    "\n",
    "        # Apply input encoders\n",
    "        u = (\n",
    "            inputs\n",
    "            if self.kernel is None\n",
    "            else tf.matmul(inputs, self.kernel, name=\"input_encoder_mult\")\n",
    "        )\n",
    "\n",
    "        if self.conv_mode == \"fft\":\n",
    "            m = self._fft_convolution(u)\n",
    "        elif self.conv_mode == \"raw\":\n",
    "            m = self._raw_convolution(u)\n",
    "\n",
    "        # apply hidden cell\n",
    "        h_in = tf.concat((m, inputs), axis=-1) if self.input_to_hidden else m\n",
    "\n",
    "        if self.hidden_cell is None:\n",
    "            h = h_in if self.return_sequences else h_in[:, -1]\n",
    "        elif hasattr(self.hidden_cell, \"state_size\"):\n",
    "            h = tf.keras.layers.RNN(\n",
    "                self.hidden_cell, return_sequences=self.return_sequences\n",
    "            )(h_in, training=training)\n",
    "        else:\n",
    "            if not self.return_sequences:\n",
    "                # no point applying the hidden cell to the whole sequence\n",
    "                h = self.hidden_cell(h_in[:, -1], training=training)\n",
    "            else:\n",
    "                h = tf.keras.layers.TimeDistributed(self.hidden_cell)(\n",
    "                    h_in, training=training\n",
    "                )\n",
    "\n",
    "        return h\n",
    "\n",
    "    def _fft_convolution(self, u):\n",
    "        seq_len = tf.shape(u)[1]\n",
    "\n",
    "        # FFT requires shape (batch, memory_d, timesteps)\n",
    "        u = tf.transpose(u, perm=[0, 2, 1])\n",
    "\n",
    "        # Pad sequences to avoid circular convolution\n",
    "        # Perform the FFT\n",
    "        fft_input = tf.signal.rfft(u, fft_length=[2 * seq_len])\n",
    "\n",
    "        # Elementwise product of FFT (with broadcasting)\n",
    "        result = tf.expand_dims(fft_input, axis=-2) * self.impulse_response\n",
    "\n",
    "        # Inverse FFT\n",
    "        m = tf.signal.irfft(result, fft_length=[2 * seq_len])[..., :seq_len]\n",
    "\n",
    "        m = tf.reshape(m, (-1, self.order * self.memory_d, seq_len))\n",
    "\n",
    "        return tf.transpose(m, perm=[0, 2, 1])\n",
    "\n",
    "    def _raw_convolution(self, u):\n",
    "        seq_len = tf.shape(u)[1]\n",
    "        ir_len = self.impulse_response.shape[0]\n",
    "\n",
    "        u = tf.expand_dims(u, -1)\n",
    "        m = tf.nn.conv2d(\n",
    "            u,\n",
    "            self.impulse_response,\n",
    "            strides=1,\n",
    "            data_format=\"NHWC\",\n",
    "            padding=[[0, 0], [ir_len - 1, 0], [0, 0], [0, 0]],\n",
    "        )\n",
    "        m = tf.reshape(m, (-1, seq_len, self.memory_d * self.order))\n",
    "        return m\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return config of layer (for serialization during model saving/loading).\"\"\"\n",
    "\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            dict(\n",
    "                memory_d=self.memory_d,\n",
    "                order=self.order,\n",
    "                theta=self.theta,\n",
    "                hidden_cell=tf.keras.layers.serialize(self.hidden_cell),\n",
    "                input_to_hidden=self.input_to_hidden,\n",
    "                discretizer=self.discretizer,\n",
    "                kernel_initializer=self.kernel_initializer,\n",
    "                dropout=self.dropout,\n",
    "                return_sequences=self.return_sequences,\n",
    "                conv_mode=self.conv_mode,\n",
    "                truncate_ir=self.truncate_ir,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Load model from serialized config.\"\"\"\n",
    "\n",
    "        config[\"hidden_cell\"] = tf.keras.layers.deserialize(config[\"hidden_cell\"])\n",
    "        return super().from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set seed to ensure the examples are reproducible\n",
    "seed = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dec = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'lmu'\n",
    "\n",
    "device = 'watch'\n",
    "subset = 2\n",
    "time_window = 2\n",
    "\n",
    "window_size = 20*time_window # 20 Hz sampling times the temporal length of the window\n",
    "\n",
    "datafile = 'data_'+device+'_subset'+str(subset)+'_'+str(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "if freq_dec:\n",
    "    model_name = \"LMU_freqdec_{}_subset{}_{}\".format(device,subset_window_size)\n",
    "else:\n",
    "    model_name = \"LMU_{}_subset{}_{}\".format(device,subset_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET NETWORK STRUCTURE PARAMETERS #####\n",
    "optim_nni_experiment = ''\n",
    "optim_nni_trial = ''\n",
    "optim_filename = 'parameter.cfg'\n",
    "optim_nni_ref = 'nni-experiments/'+optim_nni_experiment+'/trials/'+optim_nni_trial\n",
    "optim_nni_dir = os.path.expanduser('~')\n",
    "optim_filepath = os.path.join(optim_nni_dir,optim_nni_ref,optim_filename)\n",
    "\n",
    "with open(optim_filepath, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "params = json.loads(data)\n",
    "network_parameters = params['parameters']\n",
    "\n",
    "minibatch_train = network_parameters['minibatch']\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_val, x_test, y_train_oh, y_val_oh, y_test_oh) = load_wisdm2_data(datafile)\n",
    "\n",
    "if freq_dec:\n",
    "    from scipy.signal import butter\n",
    "    x_train = frequency_decomposition(x_train)\n",
    "    x_val = frequency_decomposition(x_val)\n",
    "    x_test = frequency_decomposition(x_test)\n",
    "\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "n_classes = len(y_train_oh[0])\n",
    "\n",
    "y_train = np.argmax(y_train_oh, axis=-1)\n",
    "y_val = np.argmax(y_val_oh, axis=-1)\n",
    "y_test = np.argmax(y_test_oh, axis=-1)\n",
    "\n",
    "print('timesteps:',timesteps)\n",
    "print('input_dim:',input_dim)\n",
    "print('n_classes:',n_classes)\n",
    "\n",
    "y_train = y_train[:, None, None]\n",
    "y_test = y_test[:, None, None]\n",
    "y_val = y_val[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input((timesteps,input_dim)))\n",
    "model.add(LMU(memory_d=1,\n",
    "              order=int(network_parameters['order']),\n",
    "              theta=network_parameters['theta'],\n",
    "              hidden_cell=tf.keras.layers.SimpleRNNCell(units=int(network_parameters['units'])),\n",
    "              hidden_to_memory=False,\n",
    "              memory_to_memory=True,\n",
    "              input_to_hidden=True,\n",
    "              tau=network_parameters['tau'],\n",
    "             )\n",
    "         )\n",
    "model.add(Dense(n_classes, use_bias=False))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FLOPs: {}\".format(get_flops(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = get_flops(model)*7.53e-10 # Event-Driven Signal Processing with Neuromorphic Computing Systems, https://ieeexplore.ieee.org/document/9053043/\n",
    "\n",
    "print(\"Energy evaluation on Movidius: \"+str(np.round(energy*1e6,2))+\" μJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-multimedia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
